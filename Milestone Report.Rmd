---
title: "MIlestone Report"
author: "James Thomson"
date: "November 25, 2017"
output: html_document
---

```{r setup, include=FALSE}
library(knitr)
opts_knit$set(root.dir = "C:/Users/jrtho/Documents/Coursera/Capstone/Assignment 1/final/en_US")
library(tm)
library(stringr)
library(readr)
library(ngram)
library(wordcloud)
```

# Read the data

The source dataset is available here: https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip

There are downloads for four languages - I will be using only the English datasets. It contains three files, one for Twitter, one for blogs and one for news.

```{r include=FALSE}
twit <- readLines("en_US.twitter.txt")
blog <- readLines("en_US.blogs.txt")
news <- readLines("en_US.news.txt")
```

# Summary of the data

#### File size (MB)

```{r}
size.mb <- c(file.info("en_US.twitter.txt")$size/1024^2,
             file.info("en_US.blogs.txt")$size/1024^2,
             file.info("en_US.news.txt")$size/1024^2)
```

#### Number of lines

```{r}
num.lines <- c(length(twit),
               length(blog),
               length(news))
```

#### Word count

```{r}
num.words <- c(wordcount(twit),
               wordcount(blog),
               wordcount(news))
```

#### Construct a chart of the summarized data

```{r}
source <- c("Twitter", "Blogs", "News")
data.frame(source, size.mb, num.lines, num.words)
```

Each file is quite large, between roughly 160MB and 200MB, with a significant line and word count. We will need to sample the data later for efficiency.

# Sample and cleaning the data

#### Sample the data 

With the size of the data, we will need to construct a smaller sample size to reduce computation time.

```{r}
set.seed(98765)
sample <- c(sample(twit, length(twit) * 0.001),
            sample(blog, length(blog) * 0.001),
            sample(news, length(news) * 0.001))
```

#### Load the data as a corpus

```{r}
docs <- Corpus(VectorSource(sample))
```

#### Text transformation

Text transformation

Transformation is performed using tm_map() function to replace, for example, special characters from the text.

Replacing "/", "@" and "|" with space:

```{r}
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
```

#### Cleaning the data

The tm_map() function is used to remove unnecessary white space, to convert the text to lower case, to remove common stopwords like 'the', "we". The information value of 'stopwords' is near zero due to the fact that they are so common in a language. Removing this kind of words is useful before further analyses.

```{r}
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector
docs <- tm_map(docs, removeWords, c("blabla1", "blabla2")) 
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# Text stemming
# docs <- tm_map(docs, stemDocument)
```

#### Build a term-document matrix

Document matrix is a table containing the frequency of the words. Column names are words and row names are documents. The function TermDocumentMatrix() from text mining package can be used as follow

```{r}
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
```

#### Generate word cloud

The importance of words can be illustrated as a word cloud.

```{r warning=FALSE}
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
          max.words=200, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
```

Acknowledgements: http://www.sthda.com/english/wiki/text-mining-and-word-cloud-fundamentals-in-r-5-simple-steps-you-should-know

# Next steps

- Create a prediction algorithm that takes the word or phrase entered by athe user and predicts for the most probable next phrase.
- Create a Shiny application that allows the user to input a word or phrase. The application will then predict the next word in the source text based on the algorithm decided upon.